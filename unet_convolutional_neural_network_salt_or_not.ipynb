{
  "cells": [
    {
      "metadata": {
        "_uuid": "63fd89a39309c8d67c1bded15e9df44177c70f48"
      },
      "cell_type": "markdown",
      "source": "# About my Kernel\n\nMy work is in large part from Peter HÃ¶nigschmid's kernel: [U-net, dropout, augmentation, stratification](https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification). And also Ding Li's kernel which provides more visualization of the data: [Seismic Data Analysis with U-Net](https://www.kaggle.com/dingli/seismic-data-analysis-with-u-net).\n\nCode is written in Python 3.\n\n"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a608f3489bcbe07cc013207cf0f0376d011f4d77"
      },
      "cell_type": "markdown",
      "source": "# Importing necessary packages for analysis"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport os\n\nfrom random import randint\nfrom PIL import Image\nimport glob\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization,UpSampling2D,Concatenate\n\nfrom tqdm import tqdm_notebook\nimport datetime",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c470cd8b436f68606096f1cba34fde3741e54117"
      },
      "cell_type": "markdown",
      "source": "# Loading of training/testing ids and depths\nReading the training data ID numbers and the depth values. Storing them in a DataFrame. Also create a test DataFrame with entries from depth not in train.\n\nThe training data  provided by TGS gives an excellent starting point. One of the key issues in supervised machine learning is ensuring both enough training data, and the appropriate training data. Most statistical algorithms work from having a well distributed \"randomized\" dataset, and so it is best to include many examples in our seismic data of where we have no salt, where we have a lot of salt, and everything in between. With a large enough dataset, the random data should become well-distributed. But with smaller datasets, we can often design them to be well-distributed, akin to a random dataset.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b3d276a1874903f59265b232e9682cd4eed58f3"
      },
      "cell_type": "code",
      "source": "TRAIN_IMAGE_DIR = '../input/train/images/'\nTRAIN_MASK_DIR = '../input/train/masks/'\nTEST_IMAGE_DIR = '../input/test/images/'\nMAIN_DIR = '../input/'\n\ntrain_df = pd.read_csv(MAIN_DIR + \"train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(MAIN_DIR + \"depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0aa075baf7a522a6db8da84a63874edd4a9a2c7c"
      },
      "cell_type": "code",
      "source": "train_df.head() #Showing the first few rows of the training data IDs and depths (z).",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8338ee929ef0315a1c6d028a64beba57d58a58f1"
      },
      "cell_type": "code",
      "source": "test_df.head() #Showing the first few rows of the test data IDs and depth (z)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95418c13b48929b82e0f0723e743b4f6ea86390e",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# What is the minimum and maximum pixel size of all raw PNG image files?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e17a29e450428d2332f22a38d8a20c9e65adc52b"
      },
      "cell_type": "code",
      "source": "\n# Listing the png files\ntrain_img_list = glob.glob(TRAIN_IMAGE_DIR+'*.png')\ntest_img_list = glob.glob(TEST_IMAGE_DIR+'*.png')\nmask_img_list = glob.glob(TRAIN_IMAGE_DIR+'*.png')\n\n# Opening all png files\ntrain_img_sizes = [Image.open(f, 'r').size for f in train_img_list]\ntest_img_sizes = [Image.open(f, 'r').size for f in test_img_list]\nmask_img_sizes = [Image.open(f, 'r').size for f in mask_img_list]\n\n# Printing the results of looking at the minimum and maximum image sizes\nprint(\"The minimum train image size is\", min(train_img_sizes), \"and the max is\",max(train_img_sizes))\nprint(\"The minimum test image size is\", min(test_img_sizes),\"and the max is\",max(test_img_sizes))\nprint(\"The minimum training mask image size is\", min(mask_img_sizes),\"and the max is\",max(mask_img_sizes))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5cb2fbaa1eb796002f06d24dab0217411172a94d"
      },
      "cell_type": "markdown",
      "source": "# Functions for changing image sizes from 101x101 to 128x128 to work with U-Net convolutional neural network\nIt so happens that U-Net operates in bases of 2, and does not like pixel sizes of 101x101. More on this later when we get to the convolutional neural net."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbb3b98263baa1404a242482429400fa99932f57"
      },
      "cell_type": "code",
      "source": "img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d4beb7f6abfd34d5b11b52a8f030b27de7630ba9"
      },
      "cell_type": "markdown",
      "source": "# Showing the index range of train_df\n\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a000927fea8b2b1a24a5893e4bd9a1b2d22a5f0b"
      },
      "cell_type": "code",
      "source": "print(train_df.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9d74663f577bd9f12311f71788292ccebd8aac1"
      },
      "cell_type": "markdown",
      "source": "Based on the printout of the index of train_df, we have a length of 4000, so there should be 4000 PNG files."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "099ab702e77460611aaa0733f39e545ca025e930"
      },
      "cell_type": "code",
      "source": "def filecount(dir_name):\n    # return the number of files in directory dir_name\n    try:\n        return len([f for f in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, f))])\n    except Exception:\n        return None\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "afd851114877471dc7315fccef1447957fad0f10"
      },
      "cell_type": "code",
      "source": "filecount(TRAIN_IMAGE_DIR) #Counting the files in the train image directory defined earlier.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "516c2fad3453e873498502f93c7438717f1f40cf"
      },
      "cell_type": "markdown",
      "source": "Yes, there are 4000 training image files in the directory."
    },
    {
      "metadata": {
        "_uuid": "24f534ba1fb5b59b5ba5943e39076d2907862230"
      },
      "cell_type": "markdown",
      "source": "# Amplitude value of each pixel\nEach pixel has its own color value or amplitude. The function 'load_img' is from 'keras.preprocessing.image,' and mentions the use of PIL. PIL interprets .PNG images to 256 intensities (or amplitude values) by default. Python's indices begin at 0, so the range would be 0 to 255. If we divide each pixel value by 255, we will normalize all pixel values to 1, where 0 represents completely white, and 1 represents completely black. From Pillow (PIL Fork) Documentation, Release 5.2.0:\n\"**PNG**\nbits (experimental):  For P images, this option controls how many bits to store. If omitted, the PNG writer uses 8 bits (256 colors).\"\n\n<br>\n<br>\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b73efd733f1e5620f03ab83fa017260813f3cea"
      },
      "cell_type": "code",
      "source": "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)] \n#idx stands for index,  but is just a placeholder variable. It could be anything, like \"q.\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae543b5d03c495e8ab36006a5fd2c73e1b193041"
      },
      "cell_type": "code",
      "source": "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d09ece649e8abb76bca01638dde568d8eb7e049"
      },
      "cell_type": "markdown",
      "source": "## TQDM\nIn case you're wondering, TQDM is a way to track the progress of a loop. Directly from the repository README: \"Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and you're done!\""
    },
    {
      "metadata": {
        "_uuid": "a80d04e1ca42d9038c5e30e62f9b26d61f0333db"
      },
      "cell_type": "markdown",
      "source": "### We have added 2 columns to the array train_df: \"images\" and \"masks\"\nThe columns we have added include the normalized pixel amplitude for each pixel in the image and in the mask."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c55ae67b5bf079199161fe65004980ce1763a72"
      },
      "cell_type": "code",
      "source": "train_df.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d4ce6296ca0bfae03ff82134b3d2fb6937881db",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Calculating the salt coverage and salt coverage classes\n\nAdding \"coverage\" column: Counting the number of salt pixels in the masks (all 0's or 1's) and dividing them by the image size (which is 101 pixels by 101 pixels in our case). It is useful to see what kind of distribution of coverage we have in our training (mask) data, as we should expect a similar amount of salt coverage in our predicted (mask) data.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94ab73c22a2659a9ad4206ef7ead5e594ffe77c8"
      },
      "cell_type": "code",
      "source": "train_df[\"coverage\"] = train_df.masks.map(np.sum) / (img_size_ori*img_size_ori)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64215c6b3b8607f18c3ac21feb82af5b70ccd3bc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d34775d0e4d19af55f832463a6bf3a77ab2f4d3"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(15,8))\nnum_bins = 10\nn, bins, patches = plt.hist(train_df.coverage, num_bins, facecolor='blue', alpha=0.8)\nax.set_xlabel('Mask Salt Coverage (10 bins)')\nax.set_ylabel('Total Count of Data')\nax.set_title(r'Histogram of Salt Coverage')\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d754106df704a7d164dd3476429f6130eca4e02"
      },
      "cell_type": "code",
      "source": "train_df.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23e186c0d186fac150979d3b93e0523a7525811f"
      },
      "cell_type": "code",
      "source": "train_df_dropzeros = train_df[~(train_df[['coverage']] == 0).any(axis=1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79a4f712c1de9259d73416198cebc4253144ac29"
      },
      "cell_type": "code",
      "source": "sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78899027e66dc7cc0f8cb51d28f7e92b2a12aceb"
      },
      "cell_type": "code",
      "source": "max_images = 25\ngrid_width = 5\ngrid_height = int(max_images / grid_width)*2\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax_image = axs[int(i / grid_width)*2, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(i, train_df.loc[idx].z))\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i / grid_width)*2+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.2, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(i,  round(train_df.loc[idx].coverage, 2)))\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "95b526e27758084693e6390b0d6ad4daede0e4af"
      },
      "cell_type": "markdown",
      "source": "# Create train/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb72587876d2fb47b0d2545acd574040f9e2af31"
      },
      "cell_type": "code",
      "source": "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8d5236f4f0f19a03182ab732fe1b074e9c352ab"
      },
      "cell_type": "code",
      "source": "def build_model(input_layer, start_neurons):\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    ucov1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "821ec432e3063b3453aa2768aa0311ef66bbfb86"
      },
      "cell_type": "code",
      "source": "def conv_block(m, dim, acti, bn, res, do=0):\n    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n    n = BatchNormalization()(n) if bn else n\n    n = Dropout(do)(n) if do else n\n    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n    n = BatchNormalization()(n) if bn else n\n    return Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n    if depth > 0:\n        n = conv_block(m, dim, acti, bn, res)\n        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n        if up:\n            m = UpSampling2D()(m)\n            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n        else:\n            m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n        n = Concatenate()([n, m])\n        m = conv_block(n, dim, acti, bn, res)\n    else:\n        m = conv_block(m, dim, acti, bn, res, do)\n    return m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n    i = Input(shape=img_shape)\n    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n    o = Conv2D(out_ch, 1, activation='sigmoid')(o)\n    return Model(inputs=i, outputs=o)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7990716a7f598aed35914c2020cec663268ab2ad"
      },
      "cell_type": "code",
      "source": "#model = Model(input_layer, output_layer)\nmodel = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ffeeb4d6546e468b74fb61b5710f68a035a19a7"
      },
      "cell_type": "code",
      "source": "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "058e686702c6d4372f93751b2606172e818f7120"
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78672f6d8a6ba1566eaddff73760172d05c90925"
      },
      "cell_type": "code",
      "source": "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bc49a6b7343f62911e72614debda4b9ace13ef7"
      },
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "77ae1ffc12d516e774ecc679b1e8c43dd8d78f3f"
      },
      "cell_type": "markdown",
      "source": "# Training"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "30aab6c2608f643d70d40983824cc0698cef89db"
      },
      "cell_type": "code",
      "source": "early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000005, verbose=1)\n\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f60438ce2191918e88af8ca983662fdf072ab0b3"
      },
      "cell_type": "markdown",
      "source": "# Validation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c870760f095de18587a25d3bf3b973a3011d858b"
      },
      "cell_type": "code",
      "source": "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\nax_acc.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9b22c504864ffa4b725e3fa4942e0ea888b545db"
      },
      "cell_type": "markdown",
      "source": "# Predict the validation set to do a sanity check"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bcdc261018a150b74ceb4f1653c8bc40e6e00fa"
      },
      "cell_type": "code",
      "source": "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\nmask_valid = np.array([downsample(x) for x in y_valid])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a867c8ded258d13e7c62114a5df06aa3a30ac6ad"
      },
      "cell_type": "markdown",
      "source": "# Scoring\nScore the model and do a threshold optimization by the best IoU."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "554db9ef2642dbb03a3bb123f452e7d7f886ce7d"
      },
      "cell_type": "code",
      "source": "# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "653775fb81e8d6c295a66242cb5126a28f96fd8e"
      },
      "cell_type": "code",
      "source": "thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(mask_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d70fb5e544cd431a283d542dd81c9729a3c61a0a"
      },
      "cell_type": "code",
      "source": "threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b0de75cdb92bc0b48bc3a93bc0bc82f5b82a09a"
      },
      "cell_type": "code",
      "source": "plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0311d4a0140cef5a5392e7a4f4a0bc01488734f1"
      },
      "cell_type": "markdown",
      "source": "# Sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81b4e2a09adbe55be0d8a582e34f8eeada052760"
      },
      "cell_type": "code",
      "source": "# plot small charts\nmax_images = 24\ngrid_width = 12\ngrid_height = int(max_images / grid_width)*3\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    #print(idx)\n    img = downsample(np.squeeze(x_valid[i]))\n    mask = np.squeeze(mask_valid[i])\n    pred = np.squeeze(preds_valid[i]>threshold_best)\n    ax_image = axs[int(i / grid_width)*3, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image\")\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i / grid_width)*3+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask\")\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])\n    ax_pred = axs[int(i / grid_width)*3+2, i % grid_width]\n    ax_pred.imshow(img, cmap=\"Greys\")\n    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n    coverage_pred = np.sum(pred) / pow(img_size_ori, 2)\n    ax_pred.set_title(\"Predict\")\n    ax_pred.set_yticklabels([])\n    ax_pred.set_xticklabels([])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed37ad4894dceed2efb96c7f2f3a65329b0f2192"
      },
      "cell_type": "markdown",
      "source": "# Submission\nLoad, predict and submit the test image predictions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdf6494eef5b70d4ec2bf82b16080d13e2d3cbd8"
      },
      "cell_type": "code",
      "source": "# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5468a84446ba94f4dc69d86bceeceac14fa64d4"
      },
      "cell_type": "code",
      "source": "x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9b071a9946b52b70db99f1ff2ddada0e2ec83ee"
      },
      "cell_type": "code",
      "source": "preds_test = model.predict(x_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a93995572c98f46d61b605737768390b94098b45"
      },
      "cell_type": "code",
      "source": "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bc6bfcc0b1930d1e106975a6cc33afa55b1a446"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\nprint('Submission output to: sub-{}.csv'.format(timestamp))\nsub.to_csv(\"sub-{}.csv\".format(timestamp))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73ed947a811b3d0c1cdecbb58e5eddfac09a1360"
      },
      "cell_type": "code",
      "source": "kaggle competitions submit -c tgs-salt-identification-challenge -f submission.csv -m \"Message\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "130ce6f24fed893a4750ed6f15ae0646009a922b"
      },
      "cell_type": "markdown",
      "source": "# End of workbook"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb22a37fb686f5a839e100c9b2cedaa47e555e95"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8cdfcd7a0d7cdbf640dcbc3314144bfe3fb0e00c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}